---
title: 'Group 5 - Discount in Housing Prices in Areas Exposed to Natural Disasters across the US'
author: "Gilles Mathy - Roberto Canu"
format: html
editor: visual
execute:
  warning: false    
number-sections: true
---

```{r}
library(here)
here::i_am('dm_group5.Rproj')
library(vroom)
library(readxl)
library(dplyr)
library(purrr)
library(stringr)
```

# Data Loading

After getting access to the packages that we need, we load the datasets needed for our research and all stored in the `data` folder of the project.

## [FEMA National Risk Index (NRI)](https://resilience.climate.gov/datasets/FEMA::national-risk-index-census-tracts/about)

The FEMA National Risk Index (NRI) is a nationwide dataset that estimates the relative risk of natural hazards across the U.S. It combines information on hazard frequency, exposure, expected annual losses (for buildings, people, and agriculture), and community resilience and social vulnerability.

It provides risk metrics for 18 hazard types at multiple geographic levels (county, census tract, state, and national). Included in the dataset are also the county, state, and census tract FIPS code.

The core metric of interest in the dataset is the NRI itself, which is a composite score ranging from 0 to 100 that reflects each area’s overall exposure and vulnerability to natural hazards.

```{r}
NRI_dataset <- vroom(here('data', 'NRI_dataset2.csv'))
```

We extract some specific variables we need and drop the rest: (TO FINISH)

```{r}
NRI_value <- NRI_dataset$`National Risk Index - Value - Composite`
NRI_score <- NRI_dataset$`National Risk Index - Score - Composite`
FIPS_code <- NRI_dataset$`State-County FIPS Code`
```

## [SAIPE Dataset (Poverty / Median income) - U.S. Bureau of Census](https://www.census.gov/data/datasets/2019/demo/saipe/2019-state-and-county.html)

This dataset contains the 2019 estimates for poverty and income in the United States, produced by the Small Area Income and Poverty Estimates program, often called SAIPE. It is a comprehensive dataset that includes estimates for the US, for every state, and for every county (3200 observations, of which 3144 at the county-level).

All the data files use a fixed-width format. This means the information for each geographic area is on a single line, and each piece of data is found in a specific column. The data includes several key metrics, each with its own estimate and a 90% confidence interval. The main metrics that we are going to use are: The number and percentage of people of all ages in poverty; the number and percentage of children under age 18 in poverty; the median household income. Each record ends with the area name, its two-letter state abbreviation, and a technical tag indicating the source file.

We compile the `.xsl` file we downloaded into a `.csv` to better leverage the function `vroom` and improve efficiency. We also drop the first two rows of the excel file because they present information on the dataset (which have been integrated in the description above) and prevent the right compilation in categories into the `.csv` file.

```{r}
temporary <- read_excel(here('data', 'est19all.xls'),
  sheet = 1, skip = 3)

readr::write_csv(temporary, here('data', 'SAIPE.csv'))
                 
SAIPE <- vroom(here('data', 'SAIPE.csv'))
```

Then we drop the five last columns of the loaded dataset `SAIPEI` because data is at the state-level. We also merge the FIPS codes at the state and county-level to get a unique one, since the latter is more commonly found and eases the process of merging the several datasets we use.

```{r}
SAIPE <- SAIPE[, -c(26:31)] |>
  mutate(`Complete FIPS code` = paste0(`State FIPS Code`, `County FIPS Code`)) |>
  relocate(`Complete FIPS code`, .after = 2)
```

## [Dataset on Population and Population Growth - U.S. Bureau of Census](https://www.census.gov/programs-surveys/popest/technical-documentation/research/evaluation-estimates/2020-evaluation-estimates/2010s-counties-total.html)

```{r}
population <- vroom(here('data', 'co-est2020-alldata.csv'),
                    col_types = cols(id = col_character(), 
                               date = col_date(), 
                               value = col_double()))

population <- population |>
  mutate(`Fips Code` = paste0(STATE, COUNTY)) |>
  relocate(`Fips Code`, .after = 5)
```

The data contains raw numbers for the total population and the net change by county, from 2010 to 2020. We are interested in having rates of change which can be more reliably compared. For this purpose, we create an additional variable for each year, computed as the population change in year `x` divided by the population estimate in year `x-1`. For clarity, the U.S. Bureau of Census publishes the estimates as the levels on the 1st of July of each year, so that the 2010's population estimate is the total on 1st of July 2010 and the population change in 2010 is the change between 1st of July 2009 and 1st of July 2010.

```{r}
estimates <- grep('^POPESTIMATE', names(population), value = TRUE, ignore.case = TRUE)
estimates <- estimates[-11]
changes <- grep('^NPOPCHG', names(population), value = TRUE, ignore.case = TRUE)
changes <- changes[-1]
```

```{r}
rates <- paste0('POPCHGRATE_', sub('.*?(\\d{4})$', '\\1', changes))   

population <- population |>
  mutate(!!!setNames(
      map2(changes, estimates, function(x,y) {
        net <- population[[x]]
        est <- population[[y]]
        ifelse(is.na(est) | est == 0, NA_real_, net / est)
      }),
      rates)) |>
  relocate(all_of(rates), .after = 'NPOPCHG_2020')

```

## [Dataset on Building Permits - U.S. Department of Housing and Urban Development](https://hudgis-hud.opendata.arcgis.com/datasets/HUD::residential-construction-permits-by-county/explore?filters=eyJBTExfUEVSTUlUU18yMDE5IjpbMSwzNzc1NV0sIkFMTF9QRVJNSVRTXzIwMTgiOlsxLDM0MTQ5XSwiQUxMX1BFUk1JVFNfMjAxNyI6WzEsMjU3MzFdLCJBTExfUEVSTUlUU18yMDE2IjpbMSwyNTg1Nl0sIkFMTF9QRVJNSVRTXzIwMTUiOlsxLDMzOTc1XSwiQUxMX1BFUk1JVFNfMjAxNCI6WzEsNDAwNjBdfQ%3D%3D&location=30.113599%2C-95.660185%2C4.05)

```{r}
building_permits <- vroom(here('data', 'Residential_Construction_Permits_by_County_5026727375813176131.csv'), 
              col_types = cols(id = col_character(), 
                               date = col_date(), 
                               value = col_double()))
```

NB: FIPS CODE called GEOID in this dataset.

## Zillow Home Value Index (ZHVI) Data

```{r}
zhvi_data <- vroom(here('data', 'zhvi_county.csv'))
```

# Data Cleaning 

## ZHVI Data

Creating a county code FIPS identifier
```{r}
zhvi_data <-zhvi_data %>%
  mutate(CountyCodeFIPS = paste0(StateCodeFIPS, MunicipalCodeFIPS))
```

Checking that there are no CountryCodeFIPS duplicates (making sure that the same county does not appear twice)
```{r}
sum(duplicated(zhvi_data$CountyCodeFIPS))
```

Keeping onlt relevant variables to identifying counties and the observations for each month in the year 2019
```{r}
zhvi_2019 <- zhvi_data %>%
  select(
    CountyCodeFIPS,
    State,
    starts_with("2019-")
  )
```

Checking whether there are missing values in the year 2019 overall across counties 
```{r}
any(is.na(select(zhvi_2019, starts_with("2019-"))))
```

Since there are missing observations and I want to compute a single ZHVI value for the year 2019 for each county by taking the mean of that year, I want to get an idea of whether we either have:

- Counties for which there are no observations for the year 2019 

- Or, counties have only a few missing observations 

In the former case, these counties will be dropped later. In the latter case, it is still possible to compute a relevant value by averaging only over the months for which we have observations. 

Thus, I am going to compute a variable that counts the number of NA's for each county/observation.

```{r}
zhvi_2019 <- zhvi_2019 %>%
  mutate(
    na_count_2019 = rowSums(is.na(select(., starts_with("2019-"))))
  )
```

How many counties have a full set of observations for year 2019?

```{r}
zhvi_2019 %>%
  filter(na_count_2019 == 0) %>%
  nrow()
```
How many observations have only NA's in the year 2019? 

```{r}
zhvi_2019 %>%
  filter(na_count_2019 == 12) %>%
  nrow()
```
These 68 counties will later be dropped from the final sample. 


Since we have 3073 counties in this dataset, we know that there are 3073 - (2983 + 68) = 22 counties for which we have partial observations for year 2019. 

For these 22 counties, it could be nice to identify how many months of observation are missing for them in order to get an idea of how relevant taking the average of the year 2019 will be. 

How many observations have either 1 or 2 months missing ? 

```{r}
zhvi_2019 %>% 
  filter(na_count_2019 %in% c(1, 2)) %>% 
  nrow()
```
It sounds to reasonable to still to compute the mean for the year 2019 with only the observed months for counties for which only 1 or 2 months are missing. However, computing an average for the year for counties that have more than 2 months of missing observations might not be optimal. We can thus identify these counties and drop them later. 

Which is the FIPS code of the 4 counties that have more than 2 months of missing observations? 

```{r}
zhvi_2019 %>%
  filter(na_count_2019 > 2, na_count_2019 < 12) %>%
  distinct(CountyCodeFIPS)
```

These 4 counties will have to be dropped later on, but for the time being we can compute the average ZHVI value for the year 2019 with na.rm=True to avoiding losing the 18 counties that few missing months.

Computing a single ZHVI value for each county in the dataset for the year 2019 by taking the mean value of that year 

```{r}
zhvi_2019 <- zhvi_2019 %>%
  mutate(value_2019 = rowMeans(select(.,starts_with("2019-")),na.rm = TRUE))
```


Keeping only the variables core to our analysis

```{r}
zhvi_clean <- zhvi_2019 %>%
  select(CountyCodeFIPS, State, value_2019)
```

Renaming the the average ZHVI value for the year 2019 

```{r}
zhvi_clean <- zhvi_clean %>%
  rename(avg_ZHVI_2019 = value_2019)
```


## Residential Construction Permits Dataset

Keeping only all the permits issued for each year (there is no need for further distinction in the type of permits issued for our analysis), and other relevant variables to identify the counties

```{r}
rcp <- building_permits %>%
  select(
    GEOID,
    NAME, 
    STATE_NAME,
    starts_with("ALL_PERMITS"))
```

Keeping only 5 years prior to 2019

```{r}
rcp_14_18 <- rcp %>%
  select(
    GEOID,
    NAME,
    STATE_NAME,
    matches("ALL_PERMITS_(2014|2015|2016|2017|2018)$")
  )
```

Though we are going to keep every observations in this dataset for the time being and only proceed to remove observations that contains NA's, we can already get an idea of how many counties have NA's over the time span of 2014-2018.

Let's (temporarily) create a variable that will count the NA's
```{r}
rcp_14_18 <- rcp_14_18 %>%
  mutate(
    na_count = rowSums(is.na(select(., starts_with("ALL_PERMITS"))))
  )
```


```{r}
na_count_table <- rcp_14_18 %>%
  filter(na_count %in% 0:5) %>%
  count(na_count) %>%
  arrange(na_count) %>%
  rename(
    `Number of Missing Values` = na_count,
    `Number of Counties` = n
  )

knitr::kable(
  na_count_table,
  caption = "Number of counties by number of missing values (2014–2018)"
)
```


We can also check whether we do not get any duplicates in the counties

```{r}
sum(duplicated(rcp_14_18$GEOID))
```

Cleaned dataset for the building permits

```{r}
rcp_clean <- rcp_14_18 %>%
  select(-na_count)
```











