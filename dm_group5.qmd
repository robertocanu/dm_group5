---
title: 'Group 5 - Discount in Housing Prices in Areas Exposed to Natural Disasters across the US'
author: "Gilles Mathy - Roberto Canu"
format: html
editor: visual
execute:
  warning: false    
number-sections: true
---

```{r}
library(here)
here::i_am('dm_group5.Rproj')
library(vroom)
library(readxl)
library(dplyr)
library(purrr)
library(stringr)
```

# Data Loading

After getting access to the packages that we need, we load the datasets needed for our research and all stored in the `data` folder of the project.

## [FEMA National Risk Index (NRI)](https://resilience.climate.gov/datasets/FEMA::national-risk-index-census-tracts/about)

The FEMA National Risk Index (NRI) is a nationwide dataset that estimates the relative risk of natural hazards across the U.S. It combines information on hazard frequency, exposure, expected annual losses (for buildings, people, and agriculture), and community resilience and social vulnerability.

It provides risk metrics for 18 hazard types at multiple geographic levels (county, census tract, state, and national). Included in the dataset are also the county, state, and census tract FIPS code.

The core metric of interest in the dataset is the NRI itself, which is a composite score ranging from 0 to 100 that reflects each area’s overall exposure and vulnerability to natural hazards.

```{r}
NRI_dataset <- vroom(here('data', 'NRI_dataset2.csv'))
```

We extract some specific variables we need and drop the rest: (TO FINISH)

```{r}
NRI_value <- NRI_dataset$`National Risk Index - Value - Composite`
NRI_score <- NRI_dataset$`National Risk Index - Score - Composite`
FIPS_code <- NRI_dataset$`State-County FIPS Code`
```

## [SAIPE Dataset (Poverty / Median income) - U.S. Bureau of Census](https://www.census.gov/data/datasets/2019/demo/saipe/2019-state-and-county.html)

This dataset contains the 2019 estimates for poverty and income in the United States, produced by the Small Area Income and Poverty Estimates program, often called SAIPE. It is a comprehensive dataset that includes estimates for the US, for every state, and for every county (3200 observations, of which 3144 at the county-level).

All the data files use a fixed-width format. This means the information for each geographic area is on a single line, and each piece of data is found in a specific column. The data includes several key metrics, each with its own estimate and a 90% confidence interval. The main metrics that we are going to use are: The number and percentage of people of all ages in poverty; the number and percentage of children under age 18 in poverty; the median household income. Each record ends with the area name, its two-letter state abbreviation, and a technical tag indicating the source file.

We compile the `.xsl` file we downloaded into a `.csv` to better leverage the function `vroom` and improve efficiency. We also drop the first two rows of the excel file because they present information on the dataset (which have been integrated in the description above) and prevent the right compilation in categories into the `.csv` file.

```{r}
temporary <- read_excel(here('data', 'est19all.xls'),
  sheet = 1, skip = 3)

readr::write_csv(temporary, here('data', 'SAIPE.csv'))
                 
SAIPE <- vroom(here('data', 'SAIPE.csv'))
```

Then we drop the five last columns of the loaded dataset `SAIPEI` because data is at the state-level. We also merge the FIPS codes at the state and county-level to get a unique one, since the latter is more commonly found and eases the process of merging the several datasets we use.

```{r}
SAIPE <- SAIPE[, -c(26:31)] |>
  mutate(`Complete FIPS code` = paste0(`State FIPS Code`, `County FIPS Code`)) |>
  relocate(`Complete FIPS code`, .after = 2)
```

## [Dataset on Population and Population Growth - U.S. Bureau of Census](https://www.census.gov/programs-surveys/popest/technical-documentation/research/evaluation-estimates/2020-evaluation-estimates/2010s-counties-total.html)

```{r}
population <- vroom(here('data', 'co-est2020-alldata.csv'),
                    col_types = cols(id = col_character(), 
                               date = col_date(), 
                               value = col_double()))

population <- population |>
  mutate(`Fips Code` = paste0(STATE, COUNTY)) |>
  relocate(`Fips Code`, .after = 5)
```

The data contains raw numbers for the total population and the net change by county, from 2010 to 2020. We are interested in having rates of change which can be more reliably compared. For this purpose, we create an additional variable for each year, computed as the population change in year `x` divided by the population estimate in year `x-1`. For clarity, the U.S. Bureau of Census publishes the estimates as the levels on the 1st of July of each year, so that the 2010's population estimate is the total on 1st of July 2010 and the population change in 2010 is the change between 1st of July 2009 and 1st of July 2010.

```{r}
estimates <- grep('^POPESTIMATE', names(population), value = TRUE, ignore.case = TRUE)
estimates <- estimates[-11]
changes <- grep('^NPOPCHG', names(population), value = TRUE, ignore.case = TRUE)
changes <- changes[-1]
```

```{r}
rates <- paste0('POPCHGRATE_', sub('.*?(\\d{4})$', '\\1', changes))   

population <- population |>
  mutate(!!!setNames(
      map2(changes, estimates, function(x,y) {
        net <- population[[x]]
        est <- population[[y]]
        ifelse(is.na(est) | est == 0, NA_real_, net / est)
      }),
      rates)) |>
  relocate(all_of(rates), .after = 'NPOPCHG_2020')

```

## [Dataset on Building Permits - U.S. Department of Housing and Urban Development](https://hudgis-hud.opendata.arcgis.com/datasets/HUD::residential-construction-permits-by-county/explore?filters=eyJBTExfUEVSTUlUU18yMDE5IjpbMSwzNzc1NV0sIkFMTF9QRVJNSVRTXzIwMTgiOlsxLDM0MTQ5XSwiQUxMX1BFUk1JVFNfMjAxNyI6WzEsMjU3MzFdLCJBTExfUEVSTUlUU18yMDE2IjpbMSwyNTg1Nl0sIkFMTF9QRVJNSVRTXzIwMTUiOlsxLDMzOTc1XSwiQUxMX1BFUk1JVFNfMjAxNCI6WzEsNDAwNjBdfQ%3D%3D&location=30.113599%2C-95.660185%2C4.05)

```{r}
building_permits <- vroom(here('data', 'Residential_Construction_Permits_by_County_5026727375813176131.csv'), 
              col_types = cols(id = col_character(), 
                               date = col_date(), 
                               value = col_double()))
```

NB: FIPS CODE called GEOID in this dataset.

## Zillow Home Value Index (ZHVI) Data

```{r}
zhvi_data <- vroom(here('data', 'zhvi_county.csv'))
```

# Data Cleaning 

## ZHVI Data

Creating a county code FIPS identifier
```{r}
zhvi_data <-zhvi_data %>%
  mutate(CountyCodeFIPS = paste0(StateCodeFIPS, MunicipalCodeFIPS))
```

Checking that there are no CountryCodeFIPS duplicates (making sure that the same county does not appear twice)
```{r}
sum(duplicated(zhvi_data$CountyCodeFIPS))
```

Keeping onlt relevant variables to identifying counties and the observations for each month in the year 2019
```{r}
zhvi_2019 <- zhvi_data %>%
  select(
    CountyCodeFIPS,
    State,
    starts_with("2019-")
  )
```

Checking whether there are missing values in the year 2019 overall across counties 
```{r}
any(is.na(select(zhvi_2019, starts_with("2019-"))))
```

Since there are missing observations and I want to compute a single ZHVI value for the year 2019 for each county by taking the mean of that year, I want to get an idea of whether we either have:

- Counties for which there are no observations for the year 2019 

- Or, counties have only a few missing observations 

In the former case, these counties will be dropped later. In the latter case, it is still possible to compute a relevant value by averaging only over the months for which we have observations. 

Thus, I am going to compute a variable that counts the number of NA's for each county/observation.

```{r}
zhvi_2019 <- zhvi_2019 %>%
  mutate(
    na_count_2019 = rowSums(is.na(select(., starts_with("2019-"))))
  )
```

How many counties have a full set of observations for year 2019?

```{r}
zhvi_2019 %>%
  filter(na_count_2019 == 0) %>%
  nrow()
```
How many observations have only NA's in the year 2019? 

```{r}
zhvi_2019 %>%
  filter(na_count_2019 == 12) %>%
  nrow()
```
These 68 counties will later be dropped from the final sample. 


Since we have 3073 counties in this dataset, we know that there are 3073 - (2983 + 68) = 22 counties for which we have partial observations for year 2019. 

For these 22 counties, it could be nice to identify how many months of observation are missing for them in order to get an idea of how relevant taking the average of the year 2019 will be. 

How many observations have either 1 or 2 months missing ? 

```{r}
zhvi_2019 %>% 
  filter(na_count_2019 %in% c(1, 2)) %>% 
  nrow()
```
It sounds to reasonable to still to compute the mean for the year 2019 with only the observed months for counties for which only 1 or 2 months are missing. However, computing an average for the year for counties that have more than 2 months of missing observations might not be optimal. We can thus identify these counties and drop them later. 

Which is the FIPS code of the 4 counties that have more than 2 months of missing observations? 

```{r}
zhvi_2019 %>%
  filter(na_count_2019 > 2, na_count_2019 < 12) %>%
  distinct(CountyCodeFIPS)
```

These 4 counties will have to be dropped later on, but for the time being we can compute the average ZHVI value for the year 2019 with na.rm=True to avoiding losing the 18 counties that few missing months.

Computing a single ZHVI value for each county in the dataset for the year 2019 by taking the mean value of that year 

```{r}
zhvi_2019 <- zhvi_2019 %>%
  mutate(value_2019 = rowMeans(select(.,starts_with("2019-")),na.rm = TRUE))
```


Keeping only the variables core to our analysis

```{r}
zhvi_clean <- zhvi_2019 %>%
  select(CountyCodeFIPS, State, value_2019)
```

Renaming the the average ZHVI value for the year 2019 

```{r}
zhvi_clean <- zhvi_clean %>%
  rename(avg_ZHVI_2019 = value_2019)
```


## Residential Construction Permits Dataset

Keeping only all the permits issued for each year (there is no need for further distinction in the type of permits issued for our analysis), and other relevant variables to identify the counties

```{r}
rcp <- building_permits %>%
  select(
    GEOID,
    NAME, 
    STATE_NAME,
    starts_with("ALL_PERMITS"))
```

Keeping only 5 years prior to 2019

```{r}
rcp_14_18 <- rcp %>%
  select(
    GEOID,
    NAME,
    STATE_NAME,
    matches("ALL_PERMITS_(2014|2015|2016|2017|2018)$")
  )
```

Though we are going to keep every observations in this dataset for the time being and only proceed to remove observations that contains NA's, we can already get an idea of how many counties have NA's over the time span of 2014-2018.

Let's (temporarily) create a variable that will count the NA's
```{r}
rcp_14_18 <- rcp_14_18 %>%
  mutate(
    na_count = rowSums(is.na(select(., starts_with("ALL_PERMITS"))))
  )
```


```{r}
na_count_table <- rcp_14_18 %>%
  filter(na_count %in% 0:5) %>%
  count(na_count) %>%
  arrange(na_count) %>%
  rename(
    `Number of Missing Values` = na_count,
    `Number of Counties` = n
  )

knitr::kable(
  na_count_table,
  caption = "Number of counties by number of missing values (2014–2018)"
)
```


We can also check whether we do not get any duplicates in the counties

```{r}
sum(duplicated(rcp_14_18$GEOID))
```

Cleaned dataset for the building permits

```{r}
rcp_clean <- rcp_14_18 %>%
  select(-na_count)
```


## SAIPE Dataset 

Notice that the SAIPE dataset includes FIPS codes for counties, but also 1 for the US, which is 00000, and a code for each state (e.g. 01000 for Alabama)

Keeping only the relevant variables

```{r}
SAIPE_v2 <- SAIPE %>%
  select(
    `Complete FIPS code`,
    Name,
    `Median Household Income`,
    `Poverty Estimate, All Ages`,
    `Poverty Percent, All Ages`,
    `Poverty Estimate, Age 0-17`,
    `Poverty Percent, Age 0-17`,
    `Poverty Estimate, Age 5-17 in Families`,
    `Poverty Percent, Age 5-17 in Families`
  )
```

We can check whether there are NA's in the dataset

```{r}
any(is.na(SAIPE_v2))
```

We can count how many NA's there are per rows to get an idea for how many counties we have a full set of information across all variables

```{r}
SAIPE_v2 <- SAIPE_v2 %>%
  mutate(na_count = rowSums(is.na(.)))
```

```{r}
na_count_table_SAIPE <- SAIPE_v2 %>%
  filter(na_count %in% 0:7) %>%
  count(na_count) %>%
  arrange(na_count) %>%
  rename(
    `Number of Missing Values` = na_count,
    `Number of Counties` = n
  )

knitr::kable(
  na_count_table_SAIPE,
  caption = "Number of counties by number of missing values"
)
```

It seems that there is only one county for which we have no observations at all. 

The county is the following

```{r}
SAIPE_v2 %>%
  filter(na_count == 7) %>%
  distinct(`Complete FIPS code`, Name)
```

Cleaned version of the SAIPE dataset 

```{r}
SAIPE_clean <- SAIPE_v2 %>%
  select(-na_count)
```


## NRI Dataset 

We can first try to get an idea of the missing information for the key variable of this dataset
```{r}
sum(is.na(NRI_dataset$`National Risk Index - Score - Composite`))
```
What are the different counties that have missing values in either 1 or more census tracts
```{r}
NRI_dataset %>%
  filter(is.na(`National Risk Index - Score - Composite`)) %>%
  distinct(`State-County FIPS Code`)
```
From the tibble above, we can also see that there are 89 different counties with missing value.

Each county has multiple NRI score and value. This is because it is a census-tracts level dataset. Essentially, each county is further divided into several census tracts. Thus, for our analysis, we would need to compute a county-level NRI score. This will be done by computing a population-weighted average of the census-tracts NRI scores of each county. 

For our analysis, why does the average has to be weighted for the population in each census tracts you might ask? The answer is that housing prices reflect the market of where PEOPLE LIVE, not empty forest tracts or industrial zones for instance. Thus, the NRI score of a tracts with 50,000 inhabitants should influence the county exposure (and NRI score of that county) far more than one with only 500 inhabitants. 

Let's compute the county-level NRI score using a population-weighted average

```{r}
county_nri <- NRI_dataset %>%
  group_by(`State-County FIPS Code`) %>%
  summarise(
    pop_total = sum(`Population (2020)`, na.rm = TRUE),
    nri_weighted = weighted.mean(`National Risk Index - Score - Composite`,
                                 w = `Population (2020)`,
                                 na.rm = TRUE))
```

We can also look at how many missing observations there are for the nri_weighted variable. 

```{r}
sum(is.na(county_nri$nri_weighted))
```
Due to the way nri_weighted was constructed using na.rm=TRUE, we can infer that there are 88 counties for whom there is no NRI score at the census tracts level at all. 

Since there are 89 counties that have at least 1 NA at the census tracts level for the NRI score, that means there is one county that has both missing values and observations at the census tracts level. We can identify that county. 

```{r}
counties_with_any_na <- NRI_dataset %>%
  filter(is.na(`National Risk Index - Score - Composite`)) %>%
  distinct(`State-County FIPS Code`) %>%
  pull(`State-County FIPS Code`)

counties_all_missing <- county_nri %>%
  filter(is.na(nri_weighted)) %>%
  pull(`State-County FIPS Code`)

mystery_county <- setdiff(counties_with_any_na, counties_all_missing)
mystery_county
```
```{r}
NRI_dataset %>%
  filter(`State-County FIPS Code` == mystery_county) %>%
  summarise(
    total_tracts = n(),
    n_valid = sum(!is.na(`National Risk Index - Score - Composite`)),
    n_missing = sum(is.na(`National Risk Index - Score - Composite`))
  )
```
We can see that the county with the FIPS identifier has only 1 single census tracts with a missing observation. Computing the population-weighted NRI score for that county would still be relevant. That county can be kept, meanwhile the 88 other counties will be dropped latter due to NA's. 


Notice also that at the census tracts level, there are 5 different NRI ratings used by FEMA (7 including 'No Rating' and 'Insufficient Data')
```{r}
unique(NRI_dataset$`National Risk Index - Rating - Composite`)
```

For informative purposes, as rough estimates since it is not fully representative of the way that FEMA proceeds to come up with these ratings, we could find thresholds values for our county-level NRI that corresponds to these 5 ratings by computing quantile thresholds.

```{r}
library(ggplot2)

county_nri %>%
  ggplot(aes(x = nri_weighted)) +
  geom_density(alpha = 0.6) +
  geom_rug(sides = "b") +   # little vertical marks on the x-axis
  theme_minimal() +
  labs(
    title = "Density of Population-Weighted County NRI Scores",
    x = "Population-Weighted NRI Score",
    y = "Density"
  )
```


```{r}
thresholds_county <- quantile(
  county_nri$nri_weighted,
  probs = c(.20, .40, .60, .80),
  na.rm = TRUE
)

thresholds_county
```

Now associating the county-level NRI score to a rating based on these quantile thresholds. Note that those counties that have a missing observation for the county-level NRI score will receive the "No rating". 

```{r}
county_nri <- county_nri %>%
  mutate(
    rating = case_when(
      is.na(nri_weighted) ~ "No Rating",
      nri_weighted <= thresholds_county[1] ~ "Very Low",
      nri_weighted <= thresholds_county[2] ~ "Relatively Low",
      nri_weighted <= thresholds_county[3] ~ "Relatively Moderate",
      nri_weighted <= thresholds_county[4] ~ "Relatively High",
      TRUE ~ "Very High"
    )
  )
```

To illustrate, we can also try to plot on a US map the county-level NRI ratings:

Let's first Load all US counties in 2020 since the NRI data were computed in 2020
```{r}
library(tigris)
library(sf)

options(tigris_use_cache=TRUE)
us_counties <- counties(cb=TRUE, year = 2020) %>%
  st_as_sf()
```
Preparing the county_nri dataset for merging with the map dataset
```{r}
county_nri <- county_nri %>%
  rename(GEOID = `State-County FIPS Code`)

county_nri$GEOID <- as.character(county_nri$GEOID)
```

Merging
```{r}
map_data <- us_counties %>%
  left_join(county_nri, by = "GEOID")
```


Plotting the scores
```{r}
map_conus <- st_crop(
  map_data,
  xmin = -125, xmax = -66,  # west/east
  ymin =  20,  ymax =  50   # south/north
)

ggplot(map_conus) +
  geom_sf(aes(fill = nri_weighted), color = NA) +
  scale_fill_gradient(
    name = "NRI (weighted)",
    low  = "#eff3ff",   # very light blue
    high = "#2171b5",   # moderate blue (not too dark)
    na.value = "grey90"
  ) +
  coord_sf(expand = FALSE) +
  theme_minimal() +
  labs(title = "County-Level NRI Scores")
```

A much clearer and insightful perspective could be to plot the ratings instead
```{r}
map_conus$rating <- factor(
  map_conus$rating,
  levels = c(
    "Very Low",
    "Relatively Low",
    "Relatively Moderate",
    "Relatively High",
    "Very High",
    "No Rating"
  )
)

ggplot(map_conus) +
  geom_sf(aes(fill = rating), color = NA) +
  scale_fill_manual(
    name = "Rating",
    values = c(
      "Very Low"            = "#eff3ff",
      "Relatively Low"      = "#deebf7",
      "Relatively Moderate" = "#c6dbef",
      "Relatively High"     = "#9ecae1",
      "Very High"           = "#2171b5",
      "No Rating"           = "grey90"
    ),
    na.value = "grey90"   # in case there are true NA values
  ) +
  coord_sf(expand = FALSE) +
  theme_minimal() +
  labs(title = "County-Level NRI Rating")
```

The clean version of the NRI dataset is 'county_nri'.

## Population Dataset 

Dropping all variable not interesting to our analysis
```{r}
pop_df <- population %>%
  select(1:POPCHGRATE_2020)
```

Dropping all useless geographical identifier variables
```{r}
pop_df <- pop_df %>%
  select(
    -SUMLEV,
    -REGION,
    -DIVISION,
  )
```

Are there NA's ? 
```{r}
any(is.na(pop_df))
```



# Merging the datasets

Making sure each of my dataset has the same variable name for the FIPS identifier

```{r}
zhvi_clean <- zhvi_clean %>%
  renamed(FIPS = CountyCodeFIPS)
```

```{r}
rcp_clean <- rcp_clean %>%
  rename(FIPS = GEOID)

SAIPE_clean <- SAIPE_clean %>%
  rename(FIPS = `Complete FIPS code`)
```

```{r}
nri_clean <- county_nri %>%
  rename(FIPS = GEOID)

pop_clean <- pop_df %>%
  rename(FIPS = `Fips Code`)

us_counties_clean <- us_counties %>%
  select(
    GEOID,
    NAMELSAD,
    STATE_NAME
  )

us_counties_clean <- us_counties_clean %>%
  rename(FIPS = GEOID)
```

Some datasets include both county-level and state-level data, we can remove the state-level rows

```{r}
SAIPE_clean_v2 <- SAIPE_clean %>%
  filter(!grepl("000$", FIPS))
```

```{r}
pop_clean_v2 <- pop_clean %>%
  filter(!grepl("000$", FIPS))
```

further deleting unimportant variables
```{r}
MRDY_ZHVI <- zhvi_clean %>%
  select(-State)

MRDY_RCP <- rcp_clean %>%
  select(
    -NAME,
    -STATE_NAME
  )

MRDY_SAIPE <- SAIPE_clean_v2 %>%
  select(-Name)

MRDY_NRI <- nri_clean

MRDY_POP <- pop_clean_v2 %>%
  select(
    -STATE,
    -COUNTY,
    -STNAME,
    -CTYNAME
  )
```

```{r}
MRDY_COUNTIES <- us_counties_clean %>% 
  st_drop_geometry()
```

Before merging, we want to make sure that every FIPS variable is of the same type
```{r}
MRDY_COUNTIES <- MRDY_COUNTIES %>% mutate(FIPS = as.character(FIPS))
MRDY_ZHVI     <- MRDY_ZHVI     %>% mutate(FIPS = as.character(FIPS))
MRDY_SAIPE    <- MRDY_SAIPE    %>% mutate(FIPS = as.character(FIPS))
MRDY_RCP      <- MRDY_RCP      %>% mutate(FIPS = as.character(FIPS))
MRDY_POP      <- MRDY_POP      %>% mutate(FIPS = as.character(FIPS))
MRDY_NRI      <- MRDY_NRI      %>% mutate(FIPS = as.character(FIPS))
```

We also want to check that there are no duplicates in the FIPS for the different datasets
```{r}
any(duplicated(MRDY_COUNTIES$FIPS))
any(duplicated(MRDY_ZHVI$FIPS))
any(duplicated(MRDY_SAIPE$FIPS))
any(duplicated(MRDY_RCP$FIPS))
any(duplicated(MRDY_POP$FIPS))
any(duplicated(MRDY_NRI$FIPS))
```
Using MRDY_COUNTIES as the list of all counties, we can left_join to merge everything together
```{r}
merged_df <- MRDY_COUNTIES %>%
  left_join(MRDY_ZHVI,  by = "FIPS") %>%
  left_join(MRDY_SAIPE, by = "FIPS") %>%
  left_join(MRDY_RCP,   by = "FIPS") %>%
  left_join(MRDY_POP,   by = "FIPS") %>%
  left_join(MRDY_NRI,   by = "FIPS")
```

Ordering the FIPS from lowest to highest
```{r}
final_df <- merged_df %>%
  arrange(FIPS)
```


Although further analysis is needed to determine which counties may need to be excluded from the regression due to missing values, but also to ensure that such exclusions do not bias the effects we aim to estimate, in the meantime, we can begin by checking how many fully complete rows (rows with no NA values) are available just to get a rough idea.

```{r}
sum(complete.cases(final_df))
```











